{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "!wget https://huggingface.co/lmstudio-community/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTKNoQzkp9M0",
        "outputId": "30263ade-c53d-4990-b087-27fc116026a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3485338 sha256=24c7fc76af163a21e5a631f62d0b626c44544687b682d905c2f28f6b26930bb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\n",
            "--2024-10-26 13:57:41--  https://huggingface.co/lmstudio-community/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.36.71, 3.169.36.25, 3.169.36.98, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.36.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/07/6f/076f1d80b95e6e031f137c5803483395e412be9e73c91368428db4e40aa52806/0abee8eed408d4a73a6f88e23e4ec349aef998ae0fe110e4dd1c724df5019e47?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-Q8_0.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-Q8_0.gguf%22%3B&Expires=1730210261&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDIxMDI2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA3LzZmLzA3NmYxZDgwYjk1ZTZlMDMxZjEzN2M1ODAzNDgzMzk1ZTQxMmJlOWU3M2M5MTM2ODQyOGRiNGU0MGFhNTI4MDYvMGFiZWU4ZWVkNDA4ZDRhNzNhNmY4OGUyM2U0ZWMzNDlhZWY5OThhZTBmZTExMGU0ZGQxYzcyNGRmNTAxOWU0Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XBFBxxIFJC5Qes6Pur1q6D9fJ-Crk6cX0uydFXXyRDtQCbeuGtxy-kiluykUJpvbG3mbkxPwvQuBjCk7OQq-ZpToFtYSAeO6o6v7a4MdUKgULYFZmQnH3JB-mQ5QKJ2KoJVJyviI7Ogwn1s7j0D4PTX1X1k4eQ5ecNP7htd3YBKQ74FfLikJpJmH0083-V3IAoRKMU37CgLYgMwySpLNM0iXfC%7E4QrAOfEySlCz0S%7E9lxiUesfWjF0x0aN-TUfWWYOBppnoV17EcVXHKtACsmww9hY%7E2074vUkj8As%7ELU46pWLNTUq4ou7JQReX8m3EgeUSJE31iltzplhaLNKWrJg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-10-26 13:57:41--  https://cdn-lfs-us-1.hf.co/repos/07/6f/076f1d80b95e6e031f137c5803483395e412be9e73c91368428db4e40aa52806/0abee8eed408d4a73a6f88e23e4ec349aef998ae0fe110e4dd1c724df5019e47?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-Q8_0.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-Q8_0.gguf%22%3B&Expires=1730210261&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDIxMDI2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzA3LzZmLzA3NmYxZDgwYjk1ZTZlMDMxZjEzN2M1ODAzNDgzMzk1ZTQxMmJlOWU3M2M5MTM2ODQyOGRiNGU0MGFhNTI4MDYvMGFiZWU4ZWVkNDA4ZDRhNzNhNmY4OGUyM2U0ZWMzNDlhZWY5OThhZTBmZTExMGU0ZGQxYzcyNGRmNTAxOWU0Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XBFBxxIFJC5Qes6Pur1q6D9fJ-Crk6cX0uydFXXyRDtQCbeuGtxy-kiluykUJpvbG3mbkxPwvQuBjCk7OQq-ZpToFtYSAeO6o6v7a4MdUKgULYFZmQnH3JB-mQ5QKJ2KoJVJyviI7Ogwn1s7j0D4PTX1X1k4eQ5ecNP7htd3YBKQ74FfLikJpJmH0083-V3IAoRKMU37CgLYgMwySpLNM0iXfC%7E4QrAOfEySlCz0S%7E9lxiUesfWjF0x0aN-TUfWWYOBppnoV17EcVXHKtACsmww9hY%7E2074vUkj8As%7ELU46pWLNTUq4ou7JQReX8m3EgeUSJE31iltzplhaLNKWrJg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.31, 3.169.36.128, 3.169.36.120, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1321079232 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-1B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Llama-3.2-1B-Instru 100%[===================>]   1.23G  40.8MB/s    in 31s     \n",
            "\n",
            "2024-10-26 13:58:13 (40.2 MB/s) - ‘Llama-3.2-1B-Instruct-Q8_0.gguf’ saved [1321079232/1321079232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4agkOWaHptth",
        "outputId": "a3ef48bd-cc44-4c98-f0d2-7d293e6d3d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 31 key-value pairs and 147 tensors from Llama-3.2-1B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type q8_0:  113 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 16\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 1.24 B\n",
            "llm_load_print_meta: model size       = 1.22 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 1B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.07 MiB\n",
            "llm_load_tensors:        CPU buffer size =  1252.41 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 128\n",
            "llama_new_context_with_model: n_batch    = 128\n",
            "llama_new_context_with_model: n_ubatch   = 128\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =     4.00 MiB\n",
            "llama_new_context_with_model: KV self size  =    4.00 MiB, K (f16):    2.00 MiB, V (f16):    2.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    63.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 518\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|> '+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|> ' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.key_length': '64', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 1B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '1B', 'llama.attention.value_length': '64', 'general.license': 'llama3.2', 'llama.feed_forward_length': '8192', 'llama.embedding_length': '2048', 'llama.block_count': '16', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|> '+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|> ' }}\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    38 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2316.23 ms /    39 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24456787, ' FALSE': -1.6700592}]\n",
            "0.7830428405877424 0.18823592097026215\n",
            "0.8061978410108366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1557.38 ms /    24 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20992386}]\n",
            "0.8106459634392621 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1776.05 ms /    28 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26351696}]\n",
            "0.7683445892619062 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1510.29 ms /    25 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.30694154}]\n",
            "0.7356936089980115 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1204.71 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20320746}]\n",
            "0.8161089114370109 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2836.49 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31060573}]\n",
            "0.7330028165643396 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1169.29 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.28161404, ' FALSE': -1.5387933}]\n",
            "0.7545648636870057 0.21463994579065165\n",
            "0.7785401561241431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1139.59 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.25378436, ' FALSE': -1.6394675}]\n",
            "0.7758590914257124 0.19408336847937002\n",
            "0.7999021833745037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1148.03 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3354621, ' FALSE': -1.3632808}]\n",
            "0.7150076097389183 0.25582011093586776\n",
            "0.7364927829233628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1323.26 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.19551724}]\n",
            "0.8224091635003097 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 26 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1661.92 ms /    27 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27107704, ' FALSE': -1.5787286}]\n",
            "0.762557749089508 0.20623715043424645\n",
            "0.7871199048058266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1293.01 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27320042}]\n",
            "0.7609402627755882 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2952.83 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2779461}]\n",
            "0.757337629901323 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1394.08 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3331901}]\n",
            "0.7166339395567296 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1176.25 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29611138}]\n",
            "0.7437045932388208 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1174.39 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32828486}]\n",
            "0.7201578465665478 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1428.28 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.33834094, ' FALSE': -1.3984053}]\n",
            "0.7129521739038832 0.2469905225108298\n",
            "0.7427028473331648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1148.63 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2899899, ' FALSE': -1.5047994}]\n",
            "0.7482711335581264 0.2220618425191176\n",
            "0.7711488241728681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1184.22 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27337536}]\n",
            "0.7608071558089092 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2611.54 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.22650455, ' FALSE': -1.713033}]\n",
            "0.7973157150092859 0.18031806477989076\n",
            "0.8155566342861273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1337.97 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3470459}]\n",
            "0.7067728877420559 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1057.73 ms /    17 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2753428}]\n",
            "0.759311792392917 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1420.28 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26697415}]\n",
            "0.7656928635014218 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1996.04 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.258626}]\n",
            "0.7721117280357115 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    4361.45 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29489958, ' FALSE': -1.5095412}]\n",
            "0.744606354950343 0.22101136489821968\n",
            "0.7711191910056489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 10 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2069.73 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.285061}]\n",
            "0.7519683815122274 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 10 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1287.13 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29503947, ' FALSE': -1.5046513}]\n",
            "0.7445021976882757 0.22209472297159147\n",
            "0.7702302601792131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1202.83 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.34673667}]\n",
            "0.7069914761378113 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.64 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29473346, ' FALSE': -1.4773469}]\n",
            "0.7447300578511381 0.22824243646396877\n",
            "0.7654173804526377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1205.52 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3162873}]\n",
            "0.7288500149638479 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1154.46 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23633304}]\n",
            "0.7895176872902355 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1323.79 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29444063}]\n",
            "0.744948174763519 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 26 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    3268.99 ms /    27 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26195064}]\n",
            "0.7695490062697031 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1419.05 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24768141, ' FALSE': -1.6459949}]\n",
            "0.7806085984461744 0.19282062976827052\n",
            "0.8019161288982866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1189.95 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2434201, ' FALSE': -1.6486497}]\n",
            "0.783942115011009 0.19230941020997713\n",
            "0.8030124355847273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.75 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31592652}]\n",
            "0.7291130219540046 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1351.87 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.22828369}]\n",
            "0.7958984406299284 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1169.39 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26157385}]\n",
            "0.7698390198599212 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1636.19 ms /    26 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3030907}]\n",
            "0.7385321146988236 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    3135.02 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24346559}]\n",
            "0.783906451751432 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1159.16 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.28212857}]\n",
            "0.7541767119410824 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.90 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2800833, ' FALSE': -1.5176817}]\n",
            "0.7557207882872979 0.21921951099130113\n",
            "0.7751457077387085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1296.97 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27063414}]\n",
            "0.7628955548550675 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1354.43 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23410393}]\n",
            "0.7912795715804154 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1025.52 ms /    17 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.22748277}]\n",
            "0.7965361489524835 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1193.87 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23782642}]\n",
            "0.7883395177280055 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1514.60 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26409826}]\n",
            "0.7678980847209818 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2302.26 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24178536}]\n",
            "0.7852246982395943 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1251.59 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.35726306}]\n",
            "0.6995884408941138 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1177.52 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2698101}]\n",
            "0.7635244659805467 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1636.61 ms /    26 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31939}]\n",
            "0.7265921239037515 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1322.99 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32204404}]\n",
            "0.7246662718944944 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1345.91 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32538706}]\n",
            "0.7222477458566091 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 16 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1151.29 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.25937244}]\n",
            "0.7715356165164257 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    3036.06 ms /    24 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3484301}]\n",
            "0.7057952504321743 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1323.91 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27965978}]\n",
            "0.7560409195490779 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.64 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27149975, ' FALSE': -1.5313989}]\n",
            "0.7622354717414335 0.2162329689866553\n",
            "0.7790087447013068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1132.40 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26270548}]\n",
            "0.7689683442932935 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1281.09 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26822332, ' FALSE': -1.5468831}]\n",
            "0.7647369844280624 0.2129105602830782\n",
            "0.7822215568024716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1311.36 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20714541}]\n",
            "0.8129014391868322 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1192.97 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3016632}]\n",
            "0.7395871231899938 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2834.83 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27157336}]\n",
            "0.7621793643299982 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1297.64 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26744348}]\n",
            "0.7653335875031297 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1169.51 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29193407, ' FALSE': -1.4951029}]\n",
            "0.7468177698269306 0.22422553468596176\n",
            "0.7690880173480618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1416.35 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26506993}]\n",
            "0.7671522998005406 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1198.18 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2755583}]\n",
            "0.759148177881389 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1194.26 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23157708}]\n",
            "0.793281544253401 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1174.60 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23370512}]\n",
            "0.7915952084242348 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2886.58 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24557467}]\n",
            "0.7822548721680352 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1470.60 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26976034}]\n",
            "0.7635624674460206 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 14 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1350.84 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29233313}]\n",
            "0.7465198093381393 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 16 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1155.31 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3022317}]\n",
            "0.7391667806744304 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1775.43 ms /    28 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2528179}]\n",
            "0.7766092902851626 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1177.23 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.28314313}]\n",
            "0.7534119420867348 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1311.22 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31184062}]\n",
            "0.7320981980935419 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    3473.20 ms /    26 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26163796}]\n",
            "0.769789671068783 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1318.70 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.318349}]\n",
            "0.7273488965896898 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1403.76 ms /    23 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29247862}]\n",
            "0.7464112023859135 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1759.50 ms /    28 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2166382}]\n",
            "0.8052212396693013 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 15 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.58 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.26813552}]\n",
            "0.7648041294794028 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 15 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1311.23 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2528318}]\n",
            "0.7765985049016916 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 15 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1266.77 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.22153084}]\n",
            "0.8012912101055494 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2822.87 ms /    23 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.19168012}]\n",
            "0.8255709111678887 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1282.94 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20577228}]\n",
            "0.8140184228897209 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1287.30 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.22872697}]\n",
            "0.7955457131578044 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1140.90 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20468399}]\n",
            "0.8149047943719419 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1154.35 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20224589}]\n",
            "0.8168940362760814 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1185.61 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.21122183}]\n",
            "0.8095944553861084 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1287.19 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20377149}]\n",
            "0.8156487363319135 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2830.59 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.20174435}]\n",
            "0.817303846753532 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1318.60 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.21393313}]\n",
            "0.8074023781396585 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1266.03 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3013623, ' FALSE': -1.4713242}]\n",
            "0.739809686785778 0.22962121811848185\n",
            "0.7631381288167629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1461.63 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.25369582}]\n",
            "0.775927791125726 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1542.34 ms /    24 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31053835}]\n",
            "0.7330522101945797 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1259.83 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.34048212}]\n",
            "0.7114272497898808 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1563.51 ms /    24 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3707547}]\n",
            "0.6902132377074396 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2756.72 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.28332016}]\n",
            "0.7532785805433022 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1225.45 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29181266}]\n",
            "0.7469084499584246 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1146.81 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.30676568, ' FALSE': -1.4498798}]\n",
            "0.7358230020316845 0.2345984932282056\n",
            "0.758250930782013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1292.82 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.30568716, ' FALSE': -1.4414866}]\n",
            "0.7366170272002923 0.23657580424439215\n",
            "0.7569075761756276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1147.93 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3194597}]\n",
            "0.7265414766525877 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1170.67 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.35949388, ' FALSE': -1.3026835}]\n",
            "0.6980295223638368 0.27180144180272886\n",
            "0.7197434894891149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1435.06 ms /    22 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31446385}]\n",
            "0.730180252683125 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1533.55 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32371196, ' FALSE': -1.381169}]\n",
            "0.723458596294676 0.25128463923117106\n",
            "0.7422042748564345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2373.63 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.34830564}]\n",
            "0.7058830952921936 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.83 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3118417, ' FALSE': -1.4130399}]\n",
            "0.7320974126378087 0.24340223337915007\n",
            "0.750484549765876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1270.29 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.28683612, ' FALSE': -1.5062728}]\n",
            "0.75063473464992 0.22173489157320378\n",
            "0.7719643995519831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1347.52 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.36032045}]\n",
            "0.6974527922944159 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.36 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29881087}]\n",
            "0.7416996742337774 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1148.72 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2708284}]\n",
            "0.7627473756132088 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1162.74 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.34236157, ' FALSE': -1.3467828}]\n",
            "0.7100914109589197 0.26007563054992744\n",
            "0.7319269575005907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2606.72 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32766178, ' FALSE': -1.3808081}]\n",
            "0.7206067002888608 0.2513753307805148\n",
            "0.7413786235287174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1465.69 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31121394, ' FALSE': -1.4288211}]\n",
            "0.7325571355499693 0.23959121303653672\n",
            "0.7535445969898524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1192.19 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31943014, ' FALSE': -1.4243839}]\n",
            "0.7265629563724271 0.24065669125199635\n",
            "0.7511871353698507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1157.64 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.31262368}]\n",
            "0.7315251485859696 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1411.02 ms /    23 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3897389}]\n",
            "0.6772336855612865 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1275.94 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.34678808}]\n",
            "0.7069551313429215 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1501.73 ms /    25 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.21987838}]\n",
            "0.8026164098216257 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1689.44 ms /    18 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.25725392}]\n",
            "0.7731718688486864 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2202.08 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.21046636}]\n",
            "0.8102063140170495 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 11 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1379.62 ms /    23 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.29165167}]\n",
            "0.7470287060321861 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1912.33 ms /    30 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.27242714}]\n",
            "0.761528910881454 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 16 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1281.99 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24908651}]\n",
            "0.7795125316303452 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 16 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1347.23 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.32378942, ' FALSE': -1.391176}]\n",
            "0.7234025620851154 0.24878256788658226\n",
            "0.7440995956255525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 16 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1281.71 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2522006}]\n",
            "0.7770888357035098 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    2862.71 ms /    24 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.2749899}]\n",
            "0.7595797926837915 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1672.35 ms /    25 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.24795666}]\n",
            "0.7803937621808529 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 13 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1272.69 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.3316765}]\n",
            "0.7177194582515489 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 7 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1794.87 ms /    28 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.23107396}]\n",
            "0.7936807627098632 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1047.84 ms /    17 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.18200466}]\n",
            "0.8335974555358048 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1161.56 ms /    19 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.25041705}]\n",
            "0.7784760490441271 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    2306.15 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    1372.71 ms /    20 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{' TRUE': -0.21404938}]\n",
            "0.8073085159161731 0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 12 prefix-match hit, remaining 18 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5a4d20c2397>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"____\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"____\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_relation_with_llama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Calculate normalized score as a confidence metric based on logprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a5a4d20c2397>\u001b[0m in \u001b[0;36mcheck_relation_with_llama\u001b[0;34m(subject, object)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#Defining function to check a relation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_relation_with_llama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     response, logprob_true, logprob_false = query_with_logprobs(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mMODEL_Q8_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Are the concepts '{subject}' and '{object}' directly related? Answer with one word: TRUE or FALSE.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-2-a5a4d20c2397>\u001b[0m in \u001b[0;36mquery_with_logprobs\u001b[0;34m(model, question)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_with_logprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Q: {question} A:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1898\u001b[0m             \u001b[0mResponse\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         \"\"\"\n\u001b[0;32m-> 1900\u001b[0;31m         return self.create_completion(\n\u001b[0m\u001b[1;32m   1901\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m                 token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             )\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlamaBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "#Defining models\n",
        "MODEL_Q8_0 = Llama(\n",
        "    model_path=\"Llama-3.2-1B-Instruct-Q8_0.gguf\",\n",
        "    n_ctx=128, n_gpu_layers=128, logits_all=True\n",
        ")\n",
        "\n",
        "#Defining function for getting a response\n",
        "def query_with_logprobs(model, question):\n",
        "    prompt = f\"Q: {question} A:\"\n",
        "    output = model(prompt=prompt, max_tokens=1, temperature=10000, logprobs=True)\n",
        "    response = output[\"choices\"][0]\n",
        "    text = response[\"text\"].strip().upper()\n",
        "    print(response[\"logprobs\"][\"top_logprobs\"])\n",
        "    logprobs = response[\"logprobs\"][\"top_logprobs\"][0]  # Get logprobs for first token\n",
        "\n",
        "    # Extract logprobs for TRUE and FALSE\n",
        "    logprob_true = math.exp(logprobs.get(\" TRUE\", float(\"-inf\")))\n",
        "    logprob_false = math.exp(logprobs.get(\" FALSE\", float(\"-inf\")))\n",
        "    print(logprob_true, logprob_false)\n",
        "    return text, logprob_true, logprob_false\n",
        "\n",
        "#Resolve Wikidata IDs to English labels\n",
        "df_items = pd.read_excel(\"Classeur1.xlsx\", sheet_name=\"Items\")\n",
        "df_props = pd.read_excel(\"Classeur1.xlsx\", sheet_name=\"Properties\")\n",
        "l_items = dict(zip(df_items[\"ID\"], df_items[\"Label\"]))\n",
        "l_props = dict(zip(df_props[\"ID\"], df_props[\"Label\"]))\n",
        "df_verify = pd.read_excel(\"ClinMed.xlsx\", sheet_name=\"To Add\")\n",
        "\n",
        "def get_item(x):\n",
        "    return l_items.get(x, \"____\")\n",
        "\n",
        "df_verify[\"subj\"] = df_verify[\"subject_id\"].apply(get_item)\n",
        "df_verify[\"obj\"] = df_verify[\"object_id\"].apply(get_item)\n",
        "\n",
        "#Defining function to check a relation\n",
        "def check_relation_with_llama(subject, object):\n",
        "    response, logprob_true, logprob_false = query_with_logprobs(\n",
        "        MODEL_Q8_0, f\"Are the concepts '{subject}' and '{object}' directly related? Answer with one word: TRUE or FALSE.\"\n",
        "    )\n",
        "    return response, logprob_true, logprob_false\n",
        "\n",
        "df_verify[\"llama_valid\"] = \"\"  # Initialize a new column for LLAMA validity\n",
        "\n",
        "for index, row in df_verify.iterrows():\n",
        "    subject = row[\"subj\"]\n",
        "    object = row[\"obj\"]\n",
        "\n",
        "    if subject != \"____\" and object != \"____\":\n",
        "        response, logprob_true, logprob_false = check_relation_with_llama(subject, object)\n",
        "\n",
        "        # Calculate normalized score as a confidence metric based on logprobs\n",
        "        score = (logprob_true) / (logprob_true + logprob_false)\n",
        "        df_verify.at[index, \"llama_valid\"] = score\n",
        "        print(score)\n",
        "\n",
        "df_verify.to_excel(\"verify_with_llama_add_logprobs.xlsx\", index=False)"
      ]
    }
  ]
}